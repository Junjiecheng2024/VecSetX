（当前环境是本地环境，可以修改代码，我通过GitHub手动同步到集群上，然后手动在集群上运行。本地无数据集，所以不能运行终端命令。）
我们现在正在做心脏结构的重建任务，利用VecSetX模型进行心脏结构的重建。以下为详细行动计划：

行动计划（Action Plan）

阶段 1：基础模型训练（多类重建）
目标：让模型先学会从完整的 10 类结构中进行隐式表征与重建。
步骤：

将每个病例的 10 类 segmentation mask 转换为适用于 VecSetX 的输入格式（点云表示，并附带 one-hot 类别编码）。

训练 VecSetX 的多类 autoencoder，使模型实现“输入 10 类结构，输出 10 类结构”的完整重建。

验证模型是否能够稳定地重建全心结构，确保 latent set 与 decoder 学习到了足够的心脏形状表达能力。
输出：一个能够稳定处理 10 类全心结构的 Encoder + Decoder（隐式神经场）。

阶段 2：补全任务训练（部分输入，完整输出）
目标：让模型掌握从部分结构推理完整全心结构的能力。
步骤：

在现有完整 10 类数据集上人为构造 partial masks，例如随机移除 2 至 5 个类，或按类别设计特定缺失方案。

使用“输入部分结构，输出完整结构”的方式训练模型（partial to full）。

确保模型能够根据输入的 incomplete 心脏形状推理出缺失的结构。
输出：具备补全过程能力的模型版本。

阶段 3：结构子集组合学习（subset 训练）
目标：模拟未来跨数据集场景，使模型能从不同结构子集的组合中推理完整全心。
步骤：

定义两个或多个结构子集，例如 subset1 包含 5 个结构，subset2 包含另外 5 个结构。

构造训练样本：输入 subset1 或 subset2，输出完整 10 类。

训练模型学会从不同子集组合中恢复完整心脏（例如 subset1+subset2→full）。

确认模型能在多种结构组合情况下恢复全心结构。
输出：一个能够处理多来源、不同结构组合输入的补全模型。

阶段 4：真实 partial label 学习
目标：将补全过程推广到真实只标注部分结构的数据，使模型能在实际应用中完成全心补全。
步骤：

收集真实的部分标注数据集合（例如某些病例只标注 5 类、或外部数据集标注较少的结构）。

将这些部分标注作为输入，通过前面训练过的模型进行补全训练或微调。

验证模型在真实 partial label 场景下的补全效果。
输出：一个能在真实临床或跨数据集场景中，自动从部分标注推理完整全心结构的模型。

最终目标
通过以上四个阶段，构建一个具备以下能力的系统：

能表达完整 10 类心脏结构的隐式表征模型。

能从不完整结构恢复完整心脏。

能处理跨数据集、不同结构子集的输入。

能在真实部分标注场景中实现全心补全。





---------------------------------------------------------------------------------
当前状态
数据修复已完成，且具有完整 10 类 mask，因此可直接开始阶段 1（多类重建的 VecSetX 训练）。

各类别：
1 : Myocardium : The muscle tissue surrounding the left ventricle blood pool
2 : LA : The left atrium blood pool
3 : LV : The left ventricle blood pool including the papilary muscles and trabeculation
4 : RA : The right atrium blood pool
5 : RV : The right ventricle blood pool
6 : Aorta : The aorta including the aortic cusp
7 : PA : The pulmonary artery
8 : LAA : The left atrial appendage
9 : Coronary : The left and right coronary arteries
10 : PV : The pulmonary veins


prepare_data.py数据准备脚本 (处理所有样本)（原始数据为 .nii.gz 格式的医学分割掩码，而 VecSetX 模型训练需要包含点云、SDF 值和表面法向量（可选）的 .npz 格式数据。此外，Phase 1 要求进行多类重建，因此需要在数据中包含类别信息。
网格提取: 使用 skimage.measure.marching_cubes 从分割掩码中提取 3D 网格。
表面采样: 实现了基于网格面积权重的表面点采样，确保采样点均匀分布。
类别编码: 提取了 10 个解剖结构的类别标签，并对表面点进行了 One-hot 编码。生成的表面点数据维度为 (N, 13)，其中前 3 维为坐标，后 10 维为类别标签。
SDF 计算: 使用 trimesh 和 rtree (KDTree) 计算体积点和近表面点的符号距离函数 (SDF) 值。为了提高速度，使用了法向量近似法。
归一化: 将所有点坐标归一化到 [-1, 1] 范围，与模型输入要求一致。）:
python vecset/prepare_data.py --input_dir /home/user/persistent/vecset/repaired_shape --output_dir /home/user/persistent/vecset/data_npz


create_csv.py更新数据集 CSV 文件: 
Objaverse 数据集加载器需要 CSV 文件来索引训练和验证数据。
扫描数据目录，自动生成 train.csv 和 val.csv。
数据生成完成后，请运行此命令以更新 train.csv 和 val.csv，确保新数据被包含在内：
python create_csv.py

----------------------------------------------------------------------------
正式训练环境：
CPU count	128
Logical CPU count	256
GPU count	4
GPU type	NVIDIA A100-SXM4-40GB
Python version 3.12
OS Linux-4.18.0-553.74.1.el8_10.x86_64-x86_64-with-glibc2.28
MEM: 480Gi
----------------------------------------------------------------------------
项目根目录：
/projappl/project_2016517/JunjieCheng/VecSetX

体素数据集：
/scratch/project_2016517/junjie/dataset/repaired_shape

点云数据集：（训练用数据集）
/scratch/project_2016517/junjie/dataset/repaired_npz
----------------------------------------------------------------------------
单卡测试进行中。
待单卡测试完成，运行正式4卡训练。
----------------------------------------------------------------------------
📊 训练指标详解
🎯 核心损失指标（Loss Metrics）
1. loss - 总损失（最重要的指标）
当前值: ~987 (训练), ~911 (验证)
含义: 所有损失项的加权和
计算公式: loss = loss_vol + 10 × loss_near + 0.001 × loss_eikonal + 1 × loss_surface
期望: ⬇️ 应该持续下降，表明模型在学习
2. loss_vol - 体积点 SDF 损失
当前值: ~89.9 (训练), 0.0 (验证，因为验证集没有体积点)
含义: 模型预测的体积内部/外部点的 SDF 值与真实值的差距（L1距离）
期望: ⬇️ 越小越好，表示体积重建越准确
3. loss_near - 近表面点 SDF 损失 ⭐
当前值: ~89.8 (训练), ~91.1 (验证)
含义: 模型预测的近表面点的 SDF 值与真实值的差距
重要性: ⭐⭐⭐ 权重为10，是最重要的损失项！
期望: ⬇️ 越小越好，表示表面重建越精确
4. loss_eikonal - Eikonal 正则化损失
当前值: ~0.9994
含义: SDF 梯度的范数应该接近1（数学约束）
公式: 
(||∇SDF|| - 1)²
期望: ≈ 0，当前非常接近理想值，说明梯度约束良好 ✅
5. loss_surface - 表面损失
当前值: ~0.001 (训练), ~0.0035 (验证)
含义: 表面点的 SDF 值应该为 0（因为它们在表面上）
期望: ⬇️ 越小越好，当前已经很小 ✅
📏 质量评估指标（IoU Metrics）
6. vol_iou & near_iou - 交并比
当前值: 0.0000 😟
含义: 预测的占用网格与真实占用网格的重叠度
范围: 0.0 ~ 1.0（1.0 表示完美匹配）
问题: ⚠️ IoU 为 0 说明模型还没有学到任何有意义的形状！
原因:
Epoch 0-1 刚开始训练，模型权重是随机初始化的
需要更多轮次训练才能收敛
7. 
iou
 - 综合 IoU ⭐⭐⭐
公式: 
(vol_iou + near_iou) / 2
重要性: 最重要的质量指标！
期望: ⬆️ 应该逐渐上升，目标至少 0.5+

您应该重点关注什么？
优先级 1️⃣：质量指标
✅ 主要看验证集的 iou (当前: 0.000)
   - 目标: 第10个epoch应该 > 0.1
   - 目标: 第50个epoch应该 > 0.3
   - 目标: 第200个epoch应该 > 0.5
优先级 2️⃣：损失趋势
✅ loss_near (验证集，当前: ~91)
   - 应该持续下降
   - 如果不下降或震荡，说明有问题

✅ loss (总损失，当前: ~987训练, ~911验证)
   - 应该持续下降
   - 验证loss应该接近训练loss
优先级 3️⃣：过拟合检测
⚠️ 对比训练集和验证集的 loss:
   - 训练: 987
   - 验证: 911
   - 当前验证loss更低，这很正常（验证集只用near points）
   - 如果后续训练loss远低于验证loss → 过拟合
优先级 4️⃣：系统稳定性
✅ loss_eikonal ≈ 1.0 (当前: 0.9994) ✓ 正常
✅ max mem < 40000 MB ✓ 显存充足
✅ time 稳定 ✓ 没有内存泄漏
--------------------------------------------------------------------
对于“标准化指标”（尤其是在论文中），您应该关注以下指标：

IoU（交并比）：

定义：重建结果与真实值之间的体积重叠度。

获取方式：您的训练日志中已有 test_iou（或 test_vol_iou）。

标准：对于医学器官重建，> 0.85 为优秀，> 0.7 为可接受。

Dice 系数 (DSC)：

定义：在医学成像中非常常见。Dice = 2 * IoU / (1 + IoU)。

标准：对于主要器官（左心室/右心室），通常目标值为 > 0.90；对于较小的血管，目标值为 > 0.80。

在论文中需要写：倒角距离 (CD)：
Chamfer Distance (CD):
What it is: Calculated on the surface mesh (extracted via infer.py). It measures the average distance from points on your predicted heart surface to the nearest point on the real heart surface.
Standard: The lower the better. Usually measured in millimeters (mm).
-----------------------------------------------------------------------------------
VecSetX 的工作流程
训练时（训练集样本）

# 1. 输入: 表面点云 (8192 × 13)
surface = sample_1_surface_points  # shape: (8192, 13)

# 2. Encoder: 点云 → latent set
latent_set = encoder(surface)  # shape: (16, 1024)
# 这 16 个向量是动态生成的，不是预存的！

# 3. Decoder: latent set + 查询点 → SDF
query_points = [vol_points + near_points]  # shape: (2048, 3)
sdf_pred = decoder(latent_set, query_points)  # shape: (2048,)

# 4. 计算 loss
loss = L1(sdf_pred, sdf_true)

# 5. 反向传播更新 Encoder 和 Decoder 的参数
optimizer.step()
验证时（验证集样本）

# 完全相同的流程！
surface = val_sample_surface_points  # 新样本

latent_set = encoder(surface)  # ← Encoder 可以处理新样本！
sdf_pred = decoder(latent_set, query_points)
iou = calc_iou(sdf_pred, sdf_true)
关键：Encoder 是一个可泛化的神经网络，不是查表操作！


完整的形状维度

# 输入
surface_points: (Batch, 8192, 13)
               # 8192 个点，每个点 3D 坐标 + 10 类 one-hot

# Encoder 输出
latent_set: (Batch, 16, 1024)
           # 16 个 latent vectors，每个 1024 维

# Decoder 输入
query_points: (Batch, 2048, 3)
             # 2048 个查询点 (vol + near)
latent_set: (Batch, 16, 1024)  # 重复使用

# Decoder 输出
sdf_predictions: (Batch, 2048)
                # 2048 个 SDF 值

用 16 个专家的 1024 维报告 来总结一个心脏的完整形状
---------------------------------------------------------------------------
训练后发现数据集有问题，调整数据集的计算。

发现一个问题，test_vol_iou	恒等于0.
调整数据集后发现训练还是停滞，改用run_phase1_sbatch_v2.sh，调高了学习率。

修改了数据的生成方式，现在重启训练。
现在的数据集质量：
((junjieenv) ) [chengjun@g2201 VecSetX]$ python check_data_quality.py /scratch/project_2016517/j
unjie/dataset/repaired_npz
Found 998 files in /scratch/project_2016517/junjie/dataset/repaired_npz
============================================================

============================================================
AGGREGATE STATISTICS
============================================================

📊 SDF Balance (Vol Positive %)
  Mean:   51.4%
  Std:    11.0%
  Range:  [11.2%, 78.1%]

📊 SDF Balance (Near Positive %)
  Mean:   52.9%
  Std:    0.7%
  Range:  [50.2%, 55.0%]

✅ Label Consistency
  Mean:   100.0%
  Std:    0.0%
  Range:  [100.0%, 100.0%]

📋 Class Coverage Distribution
  7 classes: 9 files (0.9%)
  8 classes: 155 files (15.5%)
  9 classes: 427 files (42.8%)
  10 classes: 407 files (40.8%)

============================================================
OVERALL ASSESSMENT
============================================================
✅ Consistency: EXCELLENT (>95%)
⚠️  Vol SDF Balance: Outside recommended range
⚠️  Near SDF Balance: Outside recommended range
⚠️  Class Coverage: 40.8% of files have all 10 classes

============================================================
Total files analyzed: 998
============================================================

训练了模型后发现还是数据集有问题。
再重新生成数据集。